
## ASCII ##
   在英文里面，字符的个数非常有限，26个字母（大小写），10个数字，加上标点符号，还有控制符，也就是键盘上所有的键所对应的字符加起来也不过是一百多个字符而已，这在计算机中用一个字节的存储空间来表示一个英文字符是绰绰有余的，因为一个字节相当于8个比特位，8个比特位可以表示256个符号。于是聪明的美国人就制定了一套字符编码的标准叫ASCII(American Standard Code for Information Interchange)，每个字符都对应一个数字，比如大写字母A对应的二进制数值是01000001。最开始ASCII只定义了128个字符编码，包括96个文字和32个控制符号，后来出现了可扩展的ASCII叫EASCII，可以用来满足一些西欧语言的字符

## GBK ##
   随着时代的进步，计算机开始普及到千家万户，比尔盖茨让每个人桌面都有一台电脑的梦想得意实现。但是计算机进入中国不得不面临的一个问题就是字符编码，虽然咱们国家的汉字是人类使用频率最多的文字，汉字博大精深，常见的汉字就有成千上万，这已经大大超出了ASCII编码所能表示的字符范围了，于是聪明的中国人自己弄了一套编码叫GB2312，这种编码收录了6763个汉字，同时他还兼容ASCII，不过GB2312还是不能100%满足中国汉字的需求，对一些罕见的字和繁体字GB2312没法处理，后来就在GB2312的基础上创建了一种叫GBK的编码，GBK不仅收录了27484个汉字，同时还收录了藏文、蒙文、维吾尔文等主要的少数民族文字。同样GBK也是兼容ASCII编码的。因此对于英文字符用1个字节来表示，汉字用两个字节来标识。

## Unicode ##
   对于如何处理中国人自己的文字我们可以另立山头，按照我们自己的需求制定一套编码规范，但是，计算机不止是美国人和中国人用啊，还有欧洲、亚洲其他国家的文字诸如日文、韩文全世界各地的文字加起来估计也有好几十万，这已经大大超出了ASCII码甚至GBK所能表示的范围了，况且人家为什么用采用你GBK标准呢？如此庞大的字符库究竟用什么方式来表示呢？于是统一联盟国际组织提出了Unicode编码，Unicode的学名是"Universal Multiple-Octet Coded Character Set"，简称为UCS。Unicode有两种格式：UCS-2和UCS-4。UCS-2就是用两个字节编码，一共16个比特位，这样理论上最多可以表示65536个字符，不过要表示全世界所有的字符显示65536个数字还远远不过，因为光汉字就有近10万个，因此Unicode4.0规范定义了一组附加的字符编码，UCS-4就是用4个字节（实际上只用了31位，最高位必须为0）。理论上完全可以涵盖一切语言所用的符号。一旦字符的Unicode编码确定下来后，就不会再改变了。但是Unicode有一定的局限性，一个Unicode字符在网络上传输或者最终存储起来的时候，并不见得每个字符都需要两个字节，比如一个ASCII字符“A“，一个字节就可以表示的字符，偏偏还要用两个字节，显然太浪费空间了。第二问题是，一个Unicode字符保存到计算机里面时就是一串01数字，那么计算机怎么知道一个2字节的Unicode字符是表示一个2字节的字符呢，还是表示两个1字节的字符呢，如果你不告诉计算机，那么计算机也会懵逼了。

“汉”字的Unicode编码是6C49，我可以用4个ascii数字来传输、保存这个编码；也可以用utf-8编码：3个连续的字节E6 B1 89来表示它。关键在于通信双方都要认可。因此Unicode编码有不同的实现方式，比如：UTF-8、UTF-16等等
汉
>>> u"汉"
u'\u6c49'

>>> u"汉".encode('utf-8')
'\xe6\xb1\x89'

unicode 符号范围
0 ~ 127  一字节
128 ~ 2047 两字节
2048 ~ 65535 三字节
65536 ~ 1114111 四字节

由上确认 6c49在三字节范围

6c49用二进制表示是： 110 110001 001001 ，填充到1110xxxx 10xxxxxx 10xxxxxx得到11100110 101110001 10001001，转换成16进制：6c49，因此“汉”的Unicode"6c49"对应的UTF-8编码是"E6B189"

Unicode/UCS只是字符集合，虽然为每个字符分配了一个唯一的整数值，但具体怎么用字节来表示每个字符，是由字符编码决定的。Unicode的字符编码方式有UTF-8, UTF-16, UTF-32。由于UTF-16和UTF-32编码中包含"\0",或者"/"这样对于文件名和其他C语言库函数来说具有特殊意义的字符，所以不适合在Unix下用来做文件名称，文本文件和环境变量的Unicode编码。UTF-8没有这样的问题，它有很多优点：可以向前兼容ASCII码，是变长的编码，由于编码没有状态，所以很容易重新同步，在传输过程中丢失了一些字节后，具有鲁棒性。



在Python中和字符串相关的数据类型，分别是str, unicode两种。它们都是basestring的子类。可见str与unicode是两种不同类型的字符串对象
In [54]: a=u'汉'
In [55]: a
Out[55]: u'\u6c49'
In [56]: type(a)
Out[56]: unicode

In [59]: b='汉'
In [60]: b
Out[60]: '\xe6\xb1\x89'
In [61]: type(b)
Out[61]: str

以上都是在Linux终端下的输出。而str类型的字符具体的编码格式是UTF-8还是GBK，还是其他的格式，是和具体的操作系统相关。

不论是Python，还是java，还是其他的编程语言，Unicode编码都成为语言默认的编码格式，而数据最后保存到介质中的时候，不同的介质有不同的方式。有人喜欢用UTF-8，有人使用GBK。这都无所谓。（终端也是一种介质）

#从str类型转换到unicode
s.decode(encoding)   =====>  <type 'str'> to <type 'unicode'>
#从unicode转换到str
u.encode(encoding)   =====>  <type 'unicode'> to <type 'str'>

所有出现乱码的原因都可以归结为字符经过不同编码解码的过程中，使用的编码格式不一致。
In [78]: a='好'
In [79]: a
Out[79]: '\xe5\xa5\xbd'
In [80]: b=a.decode('utf-8')
In [81]: b
Out[81]: u'\u597d'
In [82]: c=b.encode('gbk')
In [83]: c
Out[83]: '\xba\xc3'
In [84]: print c
��

utf-8编码的字符‘好’占用3个字节，解码成Unicode后，如果再用gbk来解码后，只有2个字节的长度了，最后出现了乱码的问题，因此防止乱码的最好方式就是始终坚持使用同一种编码格式对字符进行编码和解码操作。

